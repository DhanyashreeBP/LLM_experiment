{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/DhanyashreeBP/LLM_experiment/blob/main/Tokonizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dESrfHnNaqW9",
    "outputId": "a4d9d39d-0136-4876-fcfc-f0cd976ad79e"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers==4.42.1\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STnWZvMPd7Ul",
    "outputId": "d3ca1898-6721-4bbe-b95c-08fbfff55a05"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XR-ZIDvVeDpX",
    "outputId": "965a1345-c59d-45e3-8ba7-99c57189e069"
   },
   "outputs": [],
   "source": [
    "text = \"This is a sample sentence for word tokenization.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zififuN2ezYv",
    "outputId": "fb0d7cee-3761-46ad-8038-553bbc955068"
   },
   "outputs": [],
   "source": [
    "# This showcases word_tokenize from nltk library\n",
    "\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9M-CdtJOfENL",
    "outputId": "32c002ee-d752-4d94-f7f4-78ce57bf0fe2"
   },
   "outputs": [],
   "source": [
    "# This showcases the use of the 'spaCy' tokenizer with torchtext's get_tokenizer function\n",
    "\n",
    "text = \"I couldn't help the dog. Can't you do it? Don't be afraid if you are.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Making a list of the tokens and priting the list\n",
    "token_list = [token.text for token in doc]\n",
    "print(\"Tokens:\", token_list)\n",
    "\n",
    "# Showing token details\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hL7IHEQNheDO",
    "outputId": "fabc1500-5c99-4926-c0f9-692feefd353b"
   },
   "outputs": [],
   "source": [
    "text = \"Unicorns are real. I saw a unicorn yesterday.\"\n",
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "1ff7f2ab81224fe2a45dc1e1c6836ece",
      "736b50f157db4420852e916cc091be65",
      "7c3a5a4bbfa7461f9e61dbd416877567",
      "54d4d83127f5466e9fdeeb8105cdf574",
      "7263febf05f34c5e9278a0fc103bc644",
      "77d30342c7d5470a96ae9d619108e3c6",
      "6b4c54842b004667bd565b045588d95a",
      "ceca2bb57aa542fb92dfc18f2d22b3b3",
      "3b04312ebf5248f59b815318f95c1d60",
      "55c726bbd7784ef58fd2f96c5a945e5c",
      "2f7bfd84d4374156b7176274b4b430bc",
      "d9eb4e9a679441e0a91dfbb47a564261",
      "d2e0f301cfb84e92b8a47fca40884efe",
      "3b5f4186f84240a7a162a1e825fe2be2",
      "4e23d05e24a64f9fad55a430148d64f0",
      "849db57c3ba94305b746eabd45a36c5f",
      "6d5e631ec044437bbfb6c19b2d772e14",
      "c3db330b55694593ad93ccc3e630725c",
      "f4812423730e40bf9d5316a462ea47e6",
      "d594ed7ed2044065839018b53931eee9",
      "a74bc23193574e02acac6b6d0e044f73",
      "91960ea70123434eaa8adef8a8b1b630",
      "911a7ae3f28b48c7a97e3dc8cde36d74",
      "9bc3c03140194745a5f2362bed30f1a8",
      "7cbdbc4f789b4dcaa8687d882ab45491",
      "d61e34cf535d49adba893fb1dfe7c784",
      "2edf959469c54193bda86d30405599dc",
      "587f52f6952c418dbd1b27d386ace538",
      "375f52b56da64015a76a643d549e1818",
      "162f301884ef42b0845405b464e57709",
      "ca6205dede664acfa7644c52e84d793b",
      "95b33a25d3ff428298710fdadeeb5ce6",
      "d9a4bc95675d4a6fa809c4a9251fa28c",
      "19e64a324e8a498c97c017c25c16afb9",
      "5442942f487f472b9abf10382d60dffd",
      "667a5e35aa474de89f1098967d278127",
      "80a7a0ab3203454f857e4f2a1a613d18",
      "2a0fec15e51e400baf37cdb6468a9e4b",
      "cbd79982381d4971ad630fa3d4c2ef09",
      "abbcb7a651ca4c1eaff4b95ce061f93c",
      "aa4b3df80e07432ebb786cb6cb55178e",
      "9d8cdb3051e54847bd7bf7a103b3b165",
      "2c2bba9851dc4cae965470e41f8debb6",
      "6c28d03a967f44d5ba1f4814bc2c4cd1"
     ]
    },
    "id": "wgb_SrGBkjsx",
    "outputId": "8714bc52-0802-404e-a2b8-bbb6ff9904e7"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "fb25f52640184b3687f520eabbd778ca",
      "d580c73b9fab43ecad9ae4407d651405",
      "61862ab05a164c7c8cbc42b9473e85f8",
      "a91abc1f211144c38c5d3eccb645cfd2",
      "aca7626d48cd406793c1e3abb0af71e8",
      "e05b7fc72668476ebdd41443881ed4c2",
      "e9615691b1564bad8927319ef5982e27",
      "52c1b51bc647405abc7c30551865985e",
      "fc96c85cd3dd464c95e999fe6e7b9df2",
      "513a503ea71b4e71b0e596ea5c285910",
      "1cb0619626e44a109f380e563aedbca2",
      "684203813b8e49089fcae6d8304ff2a2",
      "ef66e381f89a4314a7410636464268cc",
      "533faf16d41a4ac2854ddb4848b819fe",
      "65e60a263e614d438ad294402044637d",
      "46fa71e01f6c4a7e822f01fd078597b5",
      "fd4d50cb1c2741fea1dd4f10296be157",
      "4b38e88a8f8d4b15bcac00b0fb9dc940",
      "5e0ed8e32f3e4006ad636112ca84971d",
      "81fc4c6060b6443da372741a0e3459ca",
      "c2b32de673644cdfb1242c5f5cc2f846",
      "745f6c2d57234623a824d3eff07e8f8a",
      "6df27706b8aa473e9c37304977b7ad01",
      "fedd581502c9477289805625ed5f1bd7",
      "5053d7fa98b74aca96d4ec30a05c67c7",
      "da469558b4f44583af095007b9ca4bfc",
      "5ad266770e6444a592782101fcc1f83c",
      "96930a00360644418ea2dd8d26107765",
      "e65791f2ae92445ea83931e8c80b344c",
      "164044ac2e3145498b28fb31754303eb",
      "fe0844e9c81c433faaacc8ac277d7bb0",
      "da1926ff18b24775abf1483d1e94883f",
      "5f562d6a581d40a6be075146a84b7568"
     ]
    },
    "id": "wjOUVwItmeVT",
    "outputId": "44ae845f-6911-49f3-f8e1-2d93bc13d343"
   },
   "outputs": [],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eLdVA505nLlY",
    "outputId": "d55e9872-403f-4750-bdaf-ea78cbb3a1a8"
   },
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokenizer(dataset[0][1])\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for  _,text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "my_iterator = yield_tokens(dataset)\n",
    "next(my_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypX7hvP0gJUv",
    "outputId": "0a1bc657-0f0f-433f-88e7-aaf1b15d5849"
   },
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "def get_tokenized_sentence_and_indices(iterator):\n",
    "    tokenized_sentence = next(iterator)  # Get the next tokenized sentence\n",
    "    token_indices = [vocab[token] for token in tokenized_sentence]  # Get token indices\n",
    "    return tokenized_sentence, token_indices\n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indices(my_iterator)\n",
    "next(my_iterator)\n",
    "\n",
    "print(\"Tokenized Sentence:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)\n",
    "\n",
    "\n",
    "lines = [\"IBM taught me tokenization\",\n",
    "         \"Special tokenizers are ready and they will blow your mind\",\n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "tokens = []\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "    tokenized_line = tokenizer_en(line)\n",
    "    tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "    tokens.append(tokenized_line)\n",
    "    max_length = max(max_length, len(tokenized_line))\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"Lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "# Build vocabulary without unk_init\n",
    "vocab = build_vocab_from_iterator(tokens, specials=['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# Vocabulary and Token Ids\n",
    "print(\"Vocabulary:\", vocab.get_itos())\n",
    "print(\"Token IDs for 'tokenization':\", vocab.get_stoi())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPrKsCXgTBfY18qjkQjPlRW",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
